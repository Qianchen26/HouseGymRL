{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d384eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== MAIN: Train + Evaluate(7 regions) + Report ======================\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- Project modules\n",
    "import config\n",
    "import evaluate\n",
    "import housegymrl\n",
    "from evaluate import create_unified_ramp, create_tasks_from_real_config\n",
    "from housegymrl import RLEnv, BaselineEnv\n",
    "\n",
    "# ---- SB3\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv, VecMonitor, VecNormalize\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 热更新非config文件\n",
    "reload(evaluate)\n",
    "reload(housegymrl)\n",
    "\n",
    "# ----------------- 全局参数 -----------------\n",
    "SEEDS = [42]\n",
    "N_ENVS = 1 # was 8\n",
    "TOTAL_STEPS = 50_000 # was 300_000\n",
    "EVAL_FREQ = 10_000\n",
    "CKPT_FREQ = 50_000\n",
    "TRAIN_REGION_KEYS = list(config.REGION_CONFIG.keys())\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "ROOT_RUNS = Path(\"runs\")\n",
    "ROOT_RUNS.mkdir(exist_ok=True, parents=True)\n",
    "try:\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "except RuntimeError:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "# ----------------- TensorBoard记录器 -----------------\n",
    "class CompletionTBCallback(BaseCallback):\n",
    "    def __init__(self, tb_every: int = 200, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        self.tb_every = int(tb_every)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        infos = self.locals.get(\"infos\", None) or []\n",
    "        \n",
    "        # 记录makespan（回合级）\n",
    "        done_days = [i.get(\"day\") for i in infos if isinstance(i, dict) and i.get(\"done\") is True]\n",
    "        if done_days:\n",
    "            self.logger.record(\"env/makespan_episode\", float(np.min(done_days)))\n",
    "            self.logger.record(\"env/episodes_finished\", float(len(done_days)))\n",
    "\n",
    "        if self.tb_every > 0 and (self.num_timesteps % self.tb_every != 0):\n",
    "            return True\n",
    "        if not infos:\n",
    "            return True\n",
    "\n",
    "        vals = [i.get(\"completion\") for i in infos if isinstance(i, dict) and (\"completion\" in i)]\n",
    "        if not vals:\n",
    "            return True\n",
    "\n",
    "        idle_vals = [i.get(\"idle_workers\", 0) for i in infos if isinstance(i, dict)]\n",
    "        K_vals = [i.get(\"K_effective\", 0) for i in infos if isinstance(i, dict)]\n",
    "        alloc_vals = [i.get(\"allocated_workers\", 0) for i in infos if isinstance(i, dict)]\n",
    "        util_vals = [a / max(1, k) for a, k in zip(alloc_vals, K_vals)] if K_vals else []\n",
    "\n",
    "        v = np.asarray(vals, dtype=float)\n",
    "        self.logger.record(\"env/completion_household_mean\", float(np.nanmean(v)))\n",
    "        self.logger.record(\"env/completion_household_min\", float(np.nanmin(v)))\n",
    "        self.logger.record(\"env/completion_household_max\", float(np.nanmax(v)))\n",
    "        self.logger.record(\"env/completion_household_p50\", float(np.nanpercentile(v, 50)))\n",
    "        self.logger.record(\"env/completion_household_p90\", float(np.nanpercentile(v, 90)))\n",
    "\n",
    "        if idle_vals:\n",
    "            self.logger.record(\"env/idle_workers_mean\", float(np.mean(idle_vals)))\n",
    "        if util_vals:\n",
    "            self.logger.record(\"env/utilization_mean\", float(np.mean(util_vals)))\n",
    "        if alloc_vals:\n",
    "            alloc_arr = np.asarray(alloc_vals, dtype=float)\n",
    "            self.logger.record(\"env/allocated_workers_p50\", float(np.percentile(alloc_arr, 50)))\n",
    "            self.logger.record(\"env/allocated_workers_p90\", float(np.percentile(alloc_arr, 90)))\n",
    "        if K_vals:\n",
    "            K_arr = np.asarray(K_vals, dtype=float)\n",
    "            self.logger.record(\"env/K_effective_p50\", float(np.percentile(K_arr, 50)))\n",
    "            self.logger.record(\"env/K_effective_p90\", float(np.percentile(K_arr, 90)))\n",
    "\n",
    "        if hasattr(self.model, \"actor\") and getattr(self.model.actor, \"optimizer\", None) is not None:\n",
    "            lr = self.model.actor.optimizer.param_groups[0][\"lr\"]\n",
    "            self.logger.record(\"train/lr\", float(lr))\n",
    "        if hasattr(self.model, \"log_ent_coef\"):\n",
    "            ent_coef = float(self.model.log_ent_coef.exp().item())\n",
    "            self.logger.record(\"train/ent_coef\", ent_coef)\n",
    "        return True\n",
    "\n",
    "# ----------------- Ramp -----------------\n",
    "ramp_fn = create_unified_ramp()\n",
    "\n",
    "# ----------------- 训练用sampler -----------------\n",
    "USE_SYNTHETIC_TRAIN = getattr(config, \"USE_SYNTHETIC_TRAIN\", False)\n",
    "\n",
    "def make_sampler(seed: int):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    def sampler():\n",
    "        region_key = rng.choice(TRAIN_REGION_KEYS)\n",
    "        cfg = config.REGION_CONFIG[region_key]\n",
    "        if USE_SYNTHETIC_TRAIN and hasattr(evaluate, \"create_tasks_from_synthetic\"):\n",
    "            tasks_df = evaluate.create_tasks_from_synthetic(cfg, rng)\n",
    "        else:\n",
    "            tasks_df = create_tasks_from_real_config(cfg, rng)\n",
    "        resources = {\"workers\": int(cfg[\"num_contractors\"]), \"region_name\": region_key}\n",
    "        return tasks_df, resources, {\"region\": region_key}\n",
    "    return sampler\n",
    "\n",
    "def make_train_env(seed: int):\n",
    "    sampler = make_sampler(seed)\n",
    "    def _init():\n",
    "        env = RLEnv(\n",
    "            scenario_sampler=sampler,\n",
    "            \n",
    "            max_steps=config.MAX_STEPS,\n",
    "            seed=seed,\n",
    "            k_ramp=ramp_fn,\n",
    "            batch_arrival=True,\n",
    "        )\n",
    "        return Monitor(env)\n",
    "    return _init\n",
    "\n",
    "def make_eval_env(region_key: str, seed: int, env_cls):\n",
    "    def _init():\n",
    "        env = evaluate.make_region_env(region_key, env_cls, k_ramp=ramp_fn, batch_arrival=True)\n",
    "        return Monitor(env)\n",
    "    return _init\n",
    "\n",
    "# =================== Train + Eval ===================\n",
    "saved_models = []\n",
    "\n",
    "for SEED in SEEDS:\n",
    "    print(f\"========== Training seed={SEED} ==========\")\n",
    "    set_random_seed(SEED)\n",
    "\n",
    "    ts_tag = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    run_dir = ROOT_RUNS / f\"sac_seed{SEED}_{ts_tag}\"\n",
    "    tb_dir = run_dir / \"tb\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 关键：为这次运行设置专属的输出目录，覆盖config中的默认路径\n",
    "    eval_output_dir = Path(\"output\") / f\"exp_{ts_tag}\"\n",
    "    (eval_output_dir / \"fig\").mkdir(parents=True, exist_ok=True)\n",
    "    (eval_output_dir / \"tab\").mkdir(parents=True, exist_ok=True)\n",
    "    config.OUTPUT_DIR = eval_output_dir\n",
    "    config.FIG_DIR = eval_output_dir / \"fig\"\n",
    "    config.TAB_DIR = eval_output_dir / \"tab\"\n",
    "\n",
    "\n",
    "    # ---- Build train vec env ----\n",
    "    env_fns = [make_train_env(SEED + i) for i in range(N_ENVS)]\n",
    "    _tmp = env_fns[0]()\n",
    "    _tmp.close()\n",
    "\n",
    "    try:\n",
    "        vec_env = SubprocVecEnv(env_fns, start_method=\"spawn\")\n",
    "    except Exception as exc:\n",
    "        print(\"[WARN] SubprocVecEnv spawn failed, fallback to DummyVecEnv:\", repr(exc))\n",
    "        vec_env = DummyVecEnv(env_fns)\n",
    "    \n",
    "    vec_env = VecMonitor(vec_env)\n",
    "    vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "\n",
    "    # ---- Eval env for callback (multi region, RL) ----\n",
    "    def make_eval_envs(seed: int):\n",
    "        envs = []\n",
    "        for i, region_key in enumerate(TRAIN_REGION_KEYS):\n",
    "            envs.append(make_eval_env(region_key, seed + 9000 + i, env_cls=RLEnv))\n",
    "        vec = DummyVecEnv(envs)\n",
    "        vec = VecMonitor(vec)\n",
    "        vec = VecNormalize(vec, norm_obs=True, norm_reward=False, training=False, clip_obs=10.0)\n",
    "        vec.training = False\n",
    "        vec.norm_reward = False\n",
    "        return vec\n",
    "\n",
    "    eval_env = make_eval_envs(SEED)\n",
    "    # 对齐统计\n",
    "    eval_env.obs_rms = vec_env.obs_rms\n",
    "    eval_env.clip_obs = vec_env.clip_obs\n",
    "    eval_env.training = False\n",
    "    eval_env.norm_reward = False\n",
    "    eval_env.ret_rms = vec_env.ret_rms\n",
    "\n",
    "    # ---- manifest ----\n",
    "    manifest = {\n",
    "        \"global_seed\": SEED,\n",
    "        \"train_env_seeds\": [SEED + i for i in range(N_ENVS)],\n",
    "        \"eval_seeds\": {region: SEED + 9000 + idx for idx, region in enumerate(TRAIN_REGION_KEYS)},\n",
    "        \"train_regions\": TRAIN_REGION_KEYS,\n",
    "        \"timestamp\": ts_tag,\n",
    "        \"output_dir\": str(config.OUTPUT_DIR),\n",
    "        \"tensorboard_dir\": str(tb_dir),\n",
    "    }\n",
    "    (run_dir / \"seed_manifest.json\").write_text(json.dumps(manifest, indent=2))\n",
    "\n",
    "    # ---- Policy/LR ----\n",
    "    total = int(TOTAL_STEPS)\n",
    "    b1, b2 = int(0.60 * total), int(0.85 * total)\n",
    "    lr1, lr2, lr3 = 3e-4, 1e-4, 5e-5\n",
    "    \n",
    "    def lr_schedule(progress_remaining: float) -> float:\n",
    "        step_done = int((1.0 - progress_remaining) * total)\n",
    "        if step_done < b1:\n",
    "            return lr1\n",
    "        if step_done < b2:\n",
    "            return lr2\n",
    "        return lr3\n",
    "\n",
    "    # 全观察架构：增大网络容量以处理 400k 维观察\n",
    "    policy_kwargs = dict(\n",
    "        net_arch=dict(\n",
    "            pi=[1024, 512],  # 策略网络：400k → 1024 → 512 → 80k\n",
    "            qf=[1024, 512],  # Q函数网络：400k → 1024 → 512 → 1\n",
    "        )\n",
    "    )\n",
    "    # 资源受限时可降低：pi/qf=[768] 或 [512]\n",
    "    batch_size = 1024 if DEVICE == \"mps\" else 512\n",
    "    \n",
    "    model = SAC(\n",
    "        \"MlpPolicy\",\n",
    "        vec_env,\n",
    "        verbose=1,\n",
    "        device=\"cpu\", # was DEVICE\n",
    "        policy_kwargs=dict(net_arch=dict(pi=[512], qf=[512])),  # 最小网络\n",
    "        # policy_kwargs=policy_kwargs,\n",
    "        learning_rate=lr_schedule,\n",
    "        buffer_size= 5000, # was max(300_000, TOTAL_STEPS),\n",
    "        batch_size=64, # was batch_size\n",
    "        gamma=0.95,\n",
    "        tau=0.01,\n",
    "        train_freq=(1, \"step\"),\n",
    "        gradient_steps=1,\n",
    "        ent_coef=\"auto\",\n",
    "        tensorboard_log=str(tb_dir),\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    eval_cb = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=str(run_dir / \"best\"),\n",
    "        log_path=str(run_dir / \"eval\"),\n",
    "        eval_freq=max(1, EVAL_FREQ // max(1, N_ENVS)),\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "    )\n",
    "    \n",
    "    ckpt_cb = CheckpointCallback(\n",
    "        save_freq=max(1, CKPT_FREQ // max(1, N_ENVS)),\n",
    "        save_path=str(run_dir / \"ckpt\"),\n",
    "        name_prefix=\"sac\",\n",
    "    )\n",
    "    \n",
    "    completion_cb = CompletionTBCallback(tb_every=200)\n",
    "\n",
    "    print(f\"[Train] Starting training for {TOTAL_STEPS} steps...\")\n",
    "    model.learn(\n",
    "        total_timesteps=TOTAL_STEPS,\n",
    "        callback=[eval_cb, ckpt_cb, completion_cb],\n",
    "        progress_bar=True\n",
    "    )\n",
    "\n",
    "    # ---- Save artifacts ----\n",
    "    model_path = run_dir / \"sac_model.zip\"\n",
    "    vecnorm_path = run_dir / \"vecnormalize.pkl\"\n",
    "    model.save(str(model_path))\n",
    "    vec_env.save(str(vecnorm_path))\n",
    "    saved_models.append(model_path)\n",
    "    \n",
    "    print(\"Saved:\", model_path)\n",
    "    print(\"VecNormalize stats:\", vecnorm_path)\n",
    "    print(\"Best checkpoint dir:\", run_dir / \"best\")\n",
    "\n",
    "    # =====================================================================\n",
    "    # 训练后评估：使用evaluate.py中更新后的evaluate_region函数\n",
    "    # 这个函数自动处理baseline（简化路径）和RL（环境step路径）\n",
    "    # =====================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"开始训练后评估（使用新的baseline评估路径）\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # 强校验动作维度\n",
    "    model_M = model.policy.action_space.shape[0]\n",
    "    assert model_M == config.MAX_HOUSES, \\\n",
    "        f\"Model action dim {model_M} != MAX_HOUSES {config.MAX_HOUSES}\"\n",
    "\n",
    "    # 确保输出目录存在\n",
    "    OUT_DIR = config.OUTPUT_DIR\n",
    "    (OUT_DIR / \"tab\").mkdir(parents=True, exist_ok=True)\n",
    "    (OUT_DIR / \"fig\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 创建TensorBoard writer\n",
    "    eval_tb_dir = OUT_DIR / \"tb\"\n",
    "    eval_tb_dir.mkdir(parents=True, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir=str(eval_tb_dir))\n",
    "    \n",
    "    # 尝试加载观测数据\n",
    "    try:\n",
    "        observed = evaluate.load_observed(config.OBSERVED_DATA_PATH)\n",
    "        print(f\"[Eval] 成功加载观测数据，包含 {len(observed)} 个地区\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Eval] 无法加载观测数据: {e}\")\n",
    "        print(\"[Eval] 将继续评估但不计算RMSE指标\")\n",
    "        observed = {}\n",
    "    \n",
    "    # 评估所有region\n",
    "    all_metrics = []\n",
    "    \n",
    "    for region_key in TRAIN_REGION_KEYS:\n",
    "        print(f\"\\n[Eval] Region = {region_key}\")\n",
    "        \n",
    "        obs_series = observed.get(region_key, pd.Series(dtype=float))\n",
    "        \n",
    "        if obs_series.size > 0:\n",
    "            print(f\"  观测数据: {len(obs_series)} 天\")\n",
    "        else:\n",
    "            print(f\"  无观测数据\")\n",
    "        \n",
    "        # 调用evaluate.py中更新后的evaluate_region函数\n",
    "        # 关键：传入训练时的vec_env作为vecnorm_src参数\n",
    "        region_metrics = evaluate.evaluate_region(\n",
    "            region=region_key,\n",
    "            obs_series=obs_series,\n",
    "            ramp_fn=ramp_fn,\n",
    "            model=model,\n",
    "            writer=writer,\n",
    "            vecnorm_src=vec_env,  # 新增：传入训练时的VecNormalize环境\n",
    "        )\n",
    "        all_metrics.extend(region_metrics)\n",
    "        \n",
    "        # 打印这个region的结果预览\n",
    "        for metric in region_metrics:\n",
    "            strategy = metric['strategy']\n",
    "            makespan = metric['makespan']\n",
    "            util = metric['utilization']\n",
    "            final = metric['final_completion']\n",
    "            print(f\"    {strategy:>8s}: makespan={makespan:>6.1f}, \"\n",
    "                  f\"util={util:.3f}, final={final:.3f}\")\n",
    "    \n",
    "    # 将结果转换为DataFrame并保存\n",
    "    df = pd.DataFrame(all_metrics)\n",
    "    df.sort_values([\"region\", \"strategy\"], inplace=True)\n",
    "    \n",
    "    metrics_path = config.TAB_DIR / \"metrics_eval.csv\"\n",
    "    df.to_csv(metrics_path, index=False)\n",
    "    print(f\"\\n[Eval] 评估指标已保存到: {metrics_path}\")\n",
    "    \n",
    "    # 生成汇总报告\n",
    "    summary_lines = [\n",
    "        \"=\"*70,\n",
    "        \"训练后评估汇总\",\n",
    "        \"=\"*70,\n",
    "        \"\",\n",
    "    ]\n",
    "    \n",
    "    for region in df[\"region\"].unique():\n",
    "        summary_lines.append(f\"\\n地区: {region}\")\n",
    "        summary_lines.append(\"-\" * 50)\n",
    "        sub = df[df[\"region\"] == region]\n",
    "        \n",
    "        for _, row in sub.iterrows():\n",
    "            line = (\n",
    "                f\"  {row['strategy']:>8s} | \"\n",
    "                f\"makespan={row['makespan']:>6.1f} | \"\n",
    "                f\"util={row['utilization']:.3f} | \"\n",
    "                f\"final={row['final_completion']:.3f}\"\n",
    "            )\n",
    "            \n",
    "            if 'rmse_aligned' in row and pd.notna(row['rmse_aligned']):\n",
    "                line += f\" | rmse_aligned={row['rmse_aligned']:.4f}\"\n",
    "            \n",
    "            summary_lines.append(line)\n",
    "    \n",
    "    summary_lines.append(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    summary_path = config.TAB_DIR / \"evaluation_summary.txt\"\n",
    "    summary_path.write_text(\"\\n\".join(summary_lines))\n",
    "    print(f\"[Eval] 汇总报告已保存到: {summary_path}\")\n",
    "    \n",
    "    # 生成策略对比可视化\n",
    "    print(\"\\n[Eval] 生成策略对比可视化...\")\n",
    "    \n",
    "    # 按策略计算平均makespan\n",
    "    strategy_means = df.groupby(\"strategy\")[\"makespan\"].mean().sort_values()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    strategy_means.plot.bar()\n",
    "    plt.ylabel(\"Average Makespan (days)\")\n",
    "    plt.title(\"平均Makespan对比（所有地区）\")\n",
    "    plt.xlabel(\"策略\")\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    overall_fig_path = config.FIG_DIR / \"overall_makespan_comparison.png\"\n",
    "    plt.savefig(overall_fig_path, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"  保存: {overall_fig_path}\")\n",
    "    \n",
    "    # 关闭TensorBoard writer\n",
    "    writer.close()\n",
    "    \n",
    "    # ---- Cleanup ----\n",
    "    try:\n",
    "        vec_env.close()\n",
    "        eval_env.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"种子 {SEED} 的训练和评估完成！\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "# =================== 最终总结 ===================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"所有种子完成！\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n已保存的模型:\")\n",
    "for path in saved_models:\n",
    "    print(f\"  - {path}\")\n",
    "\n",
    "print(f\"\\n评估报告保存位置: {config.OUTPUT_DIR}\")\n",
    "print(f\"  - 指标CSV: {config.TAB_DIR / 'metrics_eval.csv'}\")\n",
    "print(f\"  - 汇总文本: {config.TAB_DIR / 'evaluation_summary.txt'}\")\n",
    "print(f\"  - 图表: {config.FIG_DIR}\")\n",
    "print(f\"  - TensorBoard日志: {config.OUTPUT_DIR / 'tb'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"训练和评估流程全部完成！\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5c9ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urbanai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
