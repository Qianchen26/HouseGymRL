{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d384eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== MAIN: Train + Evaluate(7 regions) + Report ======================\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# ---- Project modules（不要 reload(config)，避免 OUTPUT_DIR 时间戳漂移）\n",
    "import config\n",
    "import evaluate\n",
    "import housegymrl\n",
    "from evaluate import create_unified_ramp, create_tasks_from_real_config\n",
    "from housegymrl import RLEnv, BaselineEnv  # 需已按约定实现：两个环境类\n",
    "\n",
    "# ---- SB3\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv, VecMonitor, VecNormalize\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "# 热更新非 config 文件\n",
    "reload(evaluate)\n",
    "reload(housegymrl)\n",
    "\n",
    "# ----------------- 全局参数 -----------------\n",
    "SEEDS = [42]\n",
    "N_ENVS = 8\n",
    "TOTAL_STEPS = 500_000\n",
    "EVAL_FREQ = 10_000\n",
    "CKPT_FREQ = 50_000\n",
    "TRAIN_REGION_KEYS = list(config.REGION_CONFIG.keys())  # 覆盖全部区域\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "ROOT_RUNS = Path(\"runs\"); ROOT_RUNS.mkdir(exist_ok=True, parents=True)\n",
    "try:\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "except RuntimeError:\n",
    "    pass\n",
    "\n",
    "# ----------------- TensorBoard 记录器 -----------------\n",
    "class CompletionTBCallback(BaseCallback):\n",
    "    def __init__(self, tb_every: int = 200, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        self.tb_every = int(tb_every)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        infos = self.locals.get(\"infos\", None) or []\n",
    "        # 记录 makespan（回合级，env 内已在 done=True 时写入 info['day']）\n",
    "        done_days = [i.get(\"day\") for i in infos if isinstance(i, dict) and i.get(\"done\") is True]\n",
    "        if done_days:\n",
    "            self.logger.record(\"env/makespan_episode\", float(np.min(done_days)))\n",
    "            self.logger.record(\"env/episodes_finished\", float(len(done_days)))\n",
    "\n",
    "        if self.tb_every > 0 and (self.num_timesteps % self.tb_every != 0):\n",
    "            return True\n",
    "        if not infos:\n",
    "            return True\n",
    "\n",
    "        vals = [i.get(\"completion\") for i in infos if isinstance(i, dict) and (\"completion\" in i)]\n",
    "        if not vals:\n",
    "            return True\n",
    "\n",
    "        idle_vals = [i.get(\"idle_workers\", 0) for i in infos if isinstance(i, dict)]\n",
    "        K_vals    = [i.get(\"K_effective\", 0) for i in infos if isinstance(i, dict)]\n",
    "        alloc_vals= [i.get(\"allocated_workers\", 0) for i in infos if isinstance(i, dict)]\n",
    "        util_vals = [a / max(1, k) for a, k in zip(alloc_vals, K_vals)] if K_vals else []\n",
    "\n",
    "        v = np.asarray(vals, dtype=float)\n",
    "        self.logger.record(\"env/completion_household_mean\", float(np.nanmean(v)))\n",
    "        self.logger.record(\"env/completion_household_min\",  float(np.nanmin(v)))\n",
    "        self.logger.record(\"env/completion_household_max\",  float(np.nanmax(v)))\n",
    "        self.logger.record(\"env/completion_household_p50\",  float(np.nanpercentile(v, 50)))\n",
    "        self.logger.record(\"env/completion_household_p90\",  float(np.nanpercentile(v, 90)))\n",
    "\n",
    "        if idle_vals:\n",
    "            self.logger.record(\"env/idle_workers_mean\", float(np.mean(idle_vals)))\n",
    "        if util_vals:\n",
    "            self.logger.record(\"env/utilization_mean\", float(np.mean(util_vals)))\n",
    "        if alloc_vals:\n",
    "            alloc_arr = np.asarray(alloc_vals, dtype=float)\n",
    "            self.logger.record(\"env/allocated_workers_p50\", float(np.percentile(alloc_arr, 50)))\n",
    "            self.logger.record(\"env/allocated_workers_p90\", float(np.percentile(alloc_arr, 90)))\n",
    "        if K_vals:\n",
    "            K_arr = np.asarray(K_vals, dtype=float)\n",
    "            self.logger.record(\"env/K_effective_p50\", float(np.percentile(K_arr, 50)))\n",
    "            self.logger.record(\"env/K_effective_p90\", float(np.percentile(K_arr, 90)))\n",
    "\n",
    "        if hasattr(self.model, \"actor\") and getattr(self.model.actor, \"optimizer\", None) is not None:\n",
    "            lr = self.model.actor.optimizer.param_groups[0][\"lr\"]\n",
    "            self.logger.record(\"train/lr\", float(lr))\n",
    "        if hasattr(self.model, \"log_ent_coef\"):\n",
    "            ent_coef = float(self.model.log_ent_coef.exp().item())\n",
    "            self.logger.record(\"train/ent_coef\", ent_coef)\n",
    "        return True\n",
    "\n",
    "# ----------------- Ramp -----------------\n",
    "ramp_fn = create_unified_ramp()\n",
    "\n",
    "# ----------------- 训练用 sampler -----------------\n",
    "USE_SYNTHETIC_TRAIN = getattr(config, \"USE_SYNTHETIC_TRAIN\", False)\n",
    "\n",
    "def make_sampler(seed: int):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    def sampler():\n",
    "        region_key = rng.choice(TRAIN_REGION_KEYS)\n",
    "        cfg = config.REGION_CONFIG[region_key]\n",
    "        if USE_SYNTHETIC_TRAIN and hasattr(evaluate, \"create_tasks_from_synthetic\"):\n",
    "            tasks_df = evaluate.create_tasks_from_synthetic(cfg, rng)\n",
    "        else:\n",
    "            tasks_df = create_tasks_from_real_config(cfg, rng)\n",
    "        resources = {\"workers\": int(cfg[\"num_contractors\"]), \"region_name\": region_key}\n",
    "        return tasks_df, resources, {\"region\": region_key}\n",
    "    return sampler\n",
    "\n",
    "def make_train_env(seed: int):\n",
    "    sampler = make_sampler(seed)\n",
    "    def _init():\n",
    "        env = RLEnv(\n",
    "            scenario_sampler=sampler,\n",
    "            M=config.M_CANDIDATES,\n",
    "            max_steps=config.MAX_STEPS,\n",
    "            seed=seed,\n",
    "            k_ramp=ramp_fn,\n",
    "            batch_arrival=True,\n",
    "        )\n",
    "        return Monitor(env)\n",
    "    return _init\n",
    "\n",
    "def make_eval_env(region_key: str, seed: int, env_cls):\n",
    "    def _init():\n",
    "        env = evaluate.make_region_env(region_key, env_cls, k_ramp=ramp_fn, batch_arrival=True)\n",
    "        return Monitor(env)\n",
    "    return _init\n",
    "\n",
    "# ----------------- 用训练统计包裹单环境（推理） -----------------\n",
    "def wrap_single_env_with_vecnorm(env, src_vecnorm):\n",
    "    dummy = DummyVecEnv([lambda: env])\n",
    "    v = VecNormalize(dummy, norm_obs=True, norm_reward=False, training=False, clip_obs=src_vecnorm.clip_obs)\n",
    "    v.obs_rms = src_vecnorm.obs_rms\n",
    "    v.clip_obs = src_vecnorm.clip_obs\n",
    "    v.training = False\n",
    "    v.norm_reward = False\n",
    "    v.ret_rms = src_vecnorm.ret_rms\n",
    "    return v\n",
    "\n",
    "# ----------------- 报表输出 -----------------\n",
    "def write_summary_and_plots(df: pd.DataFrame, OUT_DIR: Path):\n",
    "    TAB_DIR = OUT_DIR / \"tab\"\n",
    "    FIG_DIR = OUT_DIR / \"fig\"\n",
    "    TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    metrics_path = TAB_DIR / \"metrics_eval.csv\"\n",
    "    df.to_csv(metrics_path, index=False)\n",
    "    print(\"Saved:\", metrics_path)\n",
    "\n",
    "    agg_cols = [\"makespan\",\"auc@200\",\"auc@300\",\"t80\",\"t90\",\"t95\",\"utilization\",\"rmse_aligned\",\"rmse_overlap\",\"final_completion\"]\n",
    "    pivot_table = df.pivot_table(index=\"region\", columns=\"strategy\", values=agg_cols, aggfunc=\"first\")\n",
    "    pivot_flat = pivot_table.copy()\n",
    "    pivot_flat.columns = [f\"{m}__{s}\" for m,s in pivot_flat.columns]\n",
    "    pivot_flat = pivot_flat.reset_index()\n",
    "    summary_csv = TAB_DIR / \"summary_wide.csv\"\n",
    "    pivot_flat.to_csv(summary_csv, index=False)\n",
    "    print(\"Saved:\", summary_csv)\n",
    "\n",
    "    def barplot_metric(metric, ylabel, save_name):\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plot_df = df[[\"region\",\"strategy\",metric]].copy()\n",
    "        means = plot_df.groupby(\"strategy\")[metric].mean().sort_values(ascending=True)\n",
    "        means.plot.bar()\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title(f\"Overall {metric} by Strategy\")\n",
    "        plt.grid(axis=\"y\", alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIG_DIR / save_name, dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "    barplot_metric(\"makespan\", \"Days\", \"overall_makespan.png\")\n",
    "    barplot_metric(\"rmse_aligned\", \"RMSE\", \"overall_rmse_aligned.png\")\n",
    "    barplot_metric(\"rmse_overlap\", \"RMSE\", \"overall_rmse_overlap.png\")\n",
    "    print(\"Saved figures:\",\n",
    "          FIG_DIR / \"overall_makespan.png\",\n",
    "          FIG_DIR / \"overall_rmse_aligned.png\",\n",
    "          FIG_DIR / \"overall_rmse_overlap.png\")\n",
    "\n",
    "    # 单区对比（取第一个 region）\n",
    "    uniq = df[\"region\"].dropna().unique()\n",
    "    if len(uniq):\n",
    "        first_region = uniq[0]\n",
    "        sub = df[df[\"region\"]==first_region].set_index(\"strategy\")\n",
    "        metrics = [\"makespan\",\"auc@200\",\"utilization\",\"rmse_aligned\",\"rmse_overlap\"]\n",
    "        plt.figure(figsize=(12,6))\n",
    "        for i, m in enumerate(metrics, 1):\n",
    "            plt.subplot(1, len(metrics), i)\n",
    "            sub[m].sort_values(ascending=(m in [\"makespan\",\"rmse_aligned\",\"rmse_overlap\"])).plot.bar()\n",
    "            plt.title(m); plt.xticks(rotation=45, ha=\"right\"); plt.grid(axis=\"y\", alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        outp = (FIG_DIR / f\"{first_region}_strategy_compare.png\")\n",
    "        plt.savefig(outp, dpi=200); plt.close()\n",
    "        print(\"Saved region figure:\", outp)\n",
    "\n",
    "    # 榜单（makespan 最优策略）\n",
    "    rank_df = df.pivot_table(index=\"region\", columns=\"strategy\", values=\"makespan\", aggfunc=\"first\")\n",
    "    if not rank_df.empty:\n",
    "        rank_df[\"best_strategy\"] = rank_df.idxmin(axis=1)\n",
    "        display(rank_df.sort_index())\n",
    "\n",
    "# =================== Train + Eval ===================\n",
    "saved_models = []\n",
    "for SEED in SEEDS:\n",
    "    print(f\"========== Training seed={SEED} ==========\")\n",
    "    set_random_seed(SEED)\n",
    "\n",
    "    ts_tag = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    run_dir = ROOT_RUNS / f\"sac_seed{SEED}_{ts_tag}\"\n",
    "    tb_dir  = run_dir / \"tb\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ---- Build train vec env ----\n",
    "    env_fns = [make_train_env(SEED + i) for i in range(N_ENVS)]\n",
    "    _tmp = env_fns[0](); _tmp.close()\n",
    "\n",
    "    try:\n",
    "        vec_env = SubprocVecEnv(env_fns, start_method=\"spawn\")\n",
    "    except Exception as exc:\n",
    "        print(\"[WARN] SubprocVecEnv spawn failed, fallback to DummyVecEnv:\", repr(exc))\n",
    "        vec_env = DummyVecEnv(env_fns)\n",
    "    vec_env = VecMonitor(vec_env)\n",
    "    vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "\n",
    "    # ---- Eval env for callback (multi region, RL) ----\n",
    "    def make_eval_envs(seed: int):\n",
    "        envs = []\n",
    "        for i, region_key in enumerate(TRAIN_REGION_KEYS):\n",
    "            envs.append(make_eval_env(region_key, seed + 9000 + i, env_cls=RLEnv))\n",
    "        vec = DummyVecEnv(envs)\n",
    "        vec = VecMonitor(vec)\n",
    "        vec = VecNormalize(vec, norm_obs=True, norm_reward=False, training=False, clip_obs=10.0)\n",
    "        vec.training = False; vec.norm_reward = False\n",
    "        return vec\n",
    "\n",
    "    eval_env = make_eval_envs(SEED)\n",
    "    # 对齐统计\n",
    "    eval_env.obs_rms = vec_env.obs_rms\n",
    "    eval_env.clip_obs = vec_env.clip_obs\n",
    "    eval_env.training = False\n",
    "    eval_env.norm_reward = False\n",
    "    eval_env.ret_rms = vec_env.ret_rms\n",
    "\n",
    "    # ---- manifest ----\n",
    "    manifest = {\n",
    "        \"global_seed\": SEED,\n",
    "        \"train_env_seeds\": [SEED + i for i in range(N_ENVS)],\n",
    "        \"eval_seeds\": {region: SEED + 9000 + idx for idx, region in enumerate(TRAIN_REGION_KEYS)},\n",
    "        \"train_regions\": TRAIN_REGION_KEYS,\n",
    "        \"timestamp\": ts_tag,\n",
    "        \"output_dir\": str(config.OUTPUT_DIR),\n",
    "        \"tensorboard_dir\": str(tb_dir),\n",
    "    }\n",
    "    (run_dir / \"seed_manifest.json\").write_text(json.dumps(manifest, indent=2))\n",
    "\n",
    "    # ---- Policy/LR ----\n",
    "    total = int(TOTAL_STEPS)\n",
    "    b1, b2 = int(0.60 * total), int(0.85 * total)\n",
    "    lr1, lr2, lr3 = 3e-4, 1e-4, 5e-5\n",
    "    def lr_schedule(progress_remaining: float) -> float:\n",
    "        step_done = int((1.0 - progress_remaining) * total)\n",
    "        if step_done < b1: return lr1\n",
    "        if step_done < b2: return lr2\n",
    "        return lr3\n",
    "\n",
    "    policy_kwargs = dict(net_arch=dict(pi=[512], qf=[512]))\n",
    "    batch_size = 1024 if DEVICE == \"mps\" else 512\n",
    "    model = SAC(\n",
    "        \"MlpPolicy\",\n",
    "        vec_env,\n",
    "        verbose=1,\n",
    "        device=DEVICE,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        learning_rate=lr_schedule,\n",
    "        buffer_size=max(300_000, TOTAL_STEPS),\n",
    "        batch_size=batch_size,\n",
    "        gamma=0.95,\n",
    "        tau=0.01,\n",
    "        train_freq=(1, \"step\"),\n",
    "        gradient_steps=1,\n",
    "        ent_coef=\"auto\",\n",
    "        tensorboard_log=str(tb_dir),\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    eval_cb = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=str(run_dir / \"best\"),\n",
    "        log_path=str(run_dir / \"eval\"),\n",
    "        eval_freq=max(1, EVAL_FREQ // max(1, N_ENVS)),\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "    )\n",
    "    ckpt_cb = CheckpointCallback(\n",
    "        save_freq=max(1, CKPT_FREQ // max(1, N_ENVS)),\n",
    "        save_path=str(run_dir / \"ckpt\"),\n",
    "        name_prefix=\"sac\",\n",
    "    )\n",
    "    completion_cb = CompletionTBCallback(tb_every=200)\n",
    "\n",
    "    model.learn(total_timesteps=TOTAL_STEPS, callback=[eval_cb, ckpt_cb, completion_cb], progress_bar=True)\n",
    "\n",
    "    # ---- Save artifacts ----\n",
    "    model_path   = run_dir / \"sac_model.zip\"\n",
    "    vecnorm_path = run_dir / \"vecnormalize.pkl\"\n",
    "    model.save(str(model_path))\n",
    "    vec_env.save(str(vecnorm_path))\n",
    "    saved_models.append(model_path)\n",
    "    print(\"Saved:\", model_path)\n",
    "    print(\"VecNormalize stats:\", vecnorm_path)\n",
    "    print(\"Best checkpoint dir:\", run_dir / \"best\")\n",
    "\n",
    "    # ================= 统一评估（使用 evaluate.py 的 evaluate_region） =================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"开始评估：使用统一评估方法\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # 强校验动作维度\n",
    "    model_M = model.policy.action_space.shape[0]\n",
    "    assert model_M == config.M_CANDIDATES, f\"Model action dim {model_M} != M_CANDIDATES {config.M_CANDIDATES}\"\n",
    "\n",
    "    OUT_DIR = config.OUTPUT_DIR\n",
    "    (OUT_DIR / \"tab\").mkdir(parents=True, exist_ok=True)\n",
    "    (OUT_DIR / \"fig\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 加载观测数据（如果可用）\n",
    "    observed = {}\n",
    "    try:\n",
    "        observed = evaluate.load_observed(config.OBSERVED_DATA_PATH)\n",
    "        print(f\"✅ 加载观测数据：{len(observed)} 个区域\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 无法加载观测数据: {e}\")\n",
    "        print(\"   将继续评估但不计算 RMSE\")\n",
    "\n",
    "    # TensorBoard（评估）\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    tb_eval_dir = OUT_DIR / \"tb_eval\"\n",
    "    tb_eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "    tb_writer = SummaryWriter(log_dir=str(tb_eval_dir))\n",
    "\n",
    "    all_rows = []\n",
    "    for region_key in TRAIN_REGION_KEYS:\n",
    "        print(f\"\\n[Eval] 正在评估区域: {region_key}\")\n",
    "        rows = evaluate.evaluate_region(\n",
    "            region=region_key,\n",
    "            obs_series=observed.get(region_key, pd.Series(dtype=float)),  # 若无观测数据传空 Series\n",
    "            ramp_fn=ramp_fn,\n",
    "            model=model,\n",
    "            writer=tb_writer,        # 传入 TensorBoard writer\n",
    "            vecnorm_src=vec_env      # 传入训练时的 VecNormalize 统计\n",
    "        )\n",
    "        all_rows.extend(rows)\n",
    "        print(f\"  完成 {len(rows)} 个策略的评估\")\n",
    "\n",
    "    tb_writer.close()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"评估完成：共 {len(all_rows)} 条记录\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # 生成报表\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    write_summary_and_plots(df, OUT_DIR)\n",
    "\n",
    "    # ---- Cleanup ----\n",
    "    try:\n",
    "        vec_env.close(); eval_env.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"所有种子训练完成！\")\n",
    "print(\"=\"*70)\n",
    "print(\"保存的模型：\")\n",
    "for path in saved_models:\n",
    "    print(f\"  - {path}\")\n",
    "print(f\"\\n报表保存位置: {config.OUTPUT_DIR}\")\n",
    "print(\"=\"*70)\n",
    "# ====================== END MAIN ======================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5c9ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urbanai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
