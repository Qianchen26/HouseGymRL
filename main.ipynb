{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HouseGym RL Training with New Architecture\n",
    "\n",
    "This notebook trains RL agents on **synthetic data** and evaluates on **real data**.\n",
    "\n",
    "Key features:\n",
    "- Pure random candidate selection (M=512)\n",
    "- Batch arrival system\n",
    "- Capacity ramp system\n",
    "- Focus on robustness, not optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from importlib import reload\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Reload project modules to get latest changes\n",
    "import evaluate, baseline, housegymrl, config\n",
    "reload(evaluate); reload(baseline); reload(housegymrl); reload(config)\n",
    "\n",
    "# Import new environment classes\n",
    "from housegymrl import RLEnv, BaselineEnv\n",
    "from baseline import create_baseline_env\n",
    "from evaluate import make_synth_env\n",
    "from config import M_CANDIDATES, MAX_STEPS\n",
    "\n",
    "# SB3 imports\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv, VecMonitor, VecNormalize\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"IMPORTS COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"M_CANDIDATES = {M_CANDIDATES}\")\n",
    "print(f\"MAX_STEPS = {MAX_STEPS}\")\n",
    "print(\"Environment architecture: New (pure random candidate selection)\")\n",
    "print(\"Training data: Synthetic\")\n",
    "print(\"Testing data: Real regions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================== Training Parameters ===========================\n",
    "# For quick testing, use small values. For real training, increase these.\n",
    "\n",
    "# Quick test mode (set to False for real training)\n",
    "QUICK_TEST = True\n",
    "\n",
    "if QUICK_TEST:\n",
    "    print(\"üîß QUICK TEST MODE - Using minimal settings for functionality verification\")\n",
    "    SEEDS = [42]           # Single seed\n",
    "    N_ENVS = 2            # Just 2 parallel environments\n",
    "    TOTAL_STEPS = 1000    # Just 1000 steps (1-2 episodes)\n",
    "    EVAL_FREQ = 500       # Eval every 500 steps\n",
    "    CKPT_FREQ = 500       # Save every 500 steps\n",
    "else:\n",
    "    print(\"üöÄ FULL TRAINING MODE\")\n",
    "    SEEDS = [42]          # Can add more seeds for robustness\n",
    "    N_ENVS = 10           # 10 parallel environments\n",
    "    TOTAL_STEPS = 300_000 # 300k steps\n",
    "    EVAL_FREQ = 10_000    # Eval every 10k steps\n",
    "    CKPT_FREQ = 50_000    # Save every 50k steps\n",
    "\n",
    "# Synthetic environment parameters\n",
    "H_RANGE = (10_000, 100_000)  # House count range\n",
    "WORKER_RATIO = (0.10, 0.25)   # Contractor/house ratio range\n",
    "\n",
    "# Device selection\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Logging\n",
    "ROOT_RUNS = Path(\"runs\")\n",
    "ROOT_RUNS.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"\\nSettings:\")\n",
    "print(f\"  Seeds: {SEEDS}\")\n",
    "print(f\"  Parallel envs: {N_ENVS}\")\n",
    "print(f\"  Total steps: {TOTAL_STEPS:,}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  House range: {H_RANGE}\")\n",
    "print(f\"  Worker ratio: {WORKER_RATIO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Callback Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Custom Callbacks ========================\n",
    "\n",
    "class CompletionTBCallback(BaseCallback):\n",
    "    \"\"\"Log completion metrics to TensorBoard.\"\"\"\n",
    "    def __init__(self, tb_every: int = 200, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        self.tb_every = int(tb_every)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.tb_every > 0 and (self.num_timesteps % self.tb_every != 0):\n",
    "            return True\n",
    "        infos = self.locals.get(\"infos\", None)\n",
    "        if not infos:\n",
    "            return True\n",
    "        vals = [i.get(\"completion\") for i in infos if isinstance(i, dict) and (\"completion\" in i)]\n",
    "        if not vals:\n",
    "            return True\n",
    "        v = np.asarray(vals, dtype=float)\n",
    "        self.logger.record(\"env/completion_household_mean\", float(np.nanmean(v)))\n",
    "        self.logger.record(\"env/completion_household_min\",  float(np.nanmin(v)))\n",
    "        self.logger.record(\"env/completion_household_max\",  float(np.nanmax(v)))\n",
    "        return True\n",
    "\n",
    "class SyncVecNormCallback(BaseCallback):\n",
    "    \"\"\"Sync VecNormalize statistics between train and eval environments.\"\"\"\n",
    "    def __init__(self, src_vecnorm: VecNormalize, tgt_vecnorm: VecNormalize):\n",
    "        super().__init__(verbose=0)\n",
    "        self.src = src_vecnorm\n",
    "        self.tgt = tgt_vecnorm\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if hasattr(self.src, \"obs_rms\") and hasattr(self.tgt, \"obs_rms\"):\n",
    "            self.tgt.obs_rms = self.src.obs_rms\n",
    "        if hasattr(self.src, \"ret_rms\") and hasattr(self.tgt, \"ret_rms\"):\n",
    "            self.tgt.ret_rms = self.src.ret_rms\n",
    "        return True\n",
    "\n",
    "print(\"‚úì Callbacks defined\")\n",
    "print(\"  - CompletionTBCallback: Tracks completion metrics\")\n",
    "print(\"  - SyncVecNormCallback: Syncs normalization between train/eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Diagnostic Test - Candidate Selection & Queue Ordering\n",
    "\n",
    "This cell creates a test environment to verify:\n",
    "1. Pure random candidate selection\n",
    "2. Different ordering strategies (LJF, SJF, Random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DIAGNOSTIC: Testing Candidate Selection & Queue Ordering\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a small test environment\n",
    "test_env = make_synth_env(\n",
    "    H_min=1000, \n",
    "    H_max=2000, \n",
    "    worker_ratio=(0.15, 0.20),\n",
    "    seed=42,\n",
    "    verbose=True,\n",
    "    use_batch_arrival=True,\n",
    "    use_capacity_ramp=True\n",
    ")\n",
    "\n",
    "# Reset and get initial state\n",
    "obs, info = test_env.reset()\n",
    "\n",
    "print(f\"\\nüìä Initial State:\")\n",
    "print(f\"  Day: {test_env.day}\")\n",
    "print(f\"  Queue size: {test_env.waiting_queue.size()}\")\n",
    "print(f\"  M candidates: {test_env.M}\")\n",
    "print(f\"  Current capacity: {test_env._effective_capacity()}\")\n",
    "\n",
    "# Advance to day 50 to have some capacity\n",
    "print(\"\\n‚è© Advancing to day 50...\")\n",
    "for _ in range(50):\n",
    "    action = test_env.action_space.sample()\n",
    "    obs, r, done, trunc, info = test_env.step(action)\n",
    "    if done or trunc:\n",
    "        break\n",
    "\n",
    "print(f\"\\nüìä Day 50 State:\")\n",
    "print(f\"  Day: {test_env.day}\")\n",
    "print(f\"  Queue size: {test_env.waiting_queue.size()}\")\n",
    "print(f\"  Current capacity: {test_env._effective_capacity()}\")\n",
    "print(f\"  Completion: {info['completion']:.2%}\")\n",
    "\n",
    "# Get candidates\n",
    "queue_ids = test_env.waiting_queue.get_all()\n",
    "if len(queue_ids) > 0:\n",
    "    candidates = test_env._select_candidates(queue_ids)\n",
    "    \n",
    "    print(f\"\\nüé≤ Candidate Selection (Pure Random):\")\n",
    "    print(f\"  Queue has {len(queue_ids)} houses\")\n",
    "    print(f\"  Selected {len(candidates)} candidates\")\n",
    "    \n",
    "    # Show first 10 candidates\n",
    "    print(\"\\n  First 10 candidates:\")\n",
    "    for i in range(min(10, len(candidates))):\n",
    "        cid = candidates[i]\n",
    "        total = test_env._arr_total[cid]\n",
    "        remain = test_env._arr_rem[cid]\n",
    "        damage = test_env._arr_dmg[cid]\n",
    "        print(f\"    {i:2d}: ID={cid:4d}, Total={total:3.0f}, Remain={remain:3.0f}, Damage={damage}\")\n",
    "    \n",
    "    # Test different orderings\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Testing Different Policy Orderings\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # LJF ordering\n",
    "    ljf_sorted = sorted(candidates, key=lambda h: -test_env._arr_total[h])\n",
    "    print(\"\\nüìâ LJF Ordering (should be DESCENDING by total work):\")\n",
    "    for i in range(min(5, len(ljf_sorted))):\n",
    "        cid = ljf_sorted[i]\n",
    "        print(f\"    {i}: ID={cid:4d}, Total={test_env._arr_total[cid]:3.0f} days\")\n",
    "    \n",
    "    # SJF ordering\n",
    "    sjf_sorted = sorted(candidates, key=lambda h: test_env._arr_total[h])\n",
    "    print(\"\\nüìà SJF Ordering (should be ASCENDING by total work):\")\n",
    "    for i in range(min(5, len(sjf_sorted))):\n",
    "        cid = sjf_sorted[i]\n",
    "        print(f\"    {i}: ID={cid:4d}, Total={test_env._arr_total[cid]:3.0f} days\")\n",
    "    \n",
    "    # Random ordering (shuffle)\n",
    "    import random\n",
    "    random_sorted = candidates.copy()\n",
    "    random.shuffle(random_sorted)\n",
    "    print(\"\\nüîÄ Random Ordering (should be RANDOM):\")\n",
    "    for i in range(min(5, len(random_sorted))):\n",
    "        cid = random_sorted[i]\n",
    "        print(f\"    {i}: ID={cid:4d}, Total={test_env._arr_total[cid]:3.0f} days\")\n",
    "    \n",
    "    # RL would score these and sort by score\n",
    "    print(\"\\nü§ñ RL Ordering (would sort by learned scores):\")\n",
    "    print(\"    RL assigns scores to each candidate and sorts by score\")\n",
    "    print(\"    The scoring function is learned to maximize long-term reward\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Queue is empty, cannot test ordering\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Diagnostic test complete\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Environment Factory Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Environment Factories ========================\n",
    "\n",
    "# Set multiprocessing start method (for macOS compatibility)\n",
    "try:\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "except RuntimeError:\n",
    "    pass\n",
    "\n",
    "def _make_env_worker(H_min: int, H_max: int, worker_ratio, seed: int, rank: int):\n",
    "    \"\"\"Create a single synthetic environment for training.\"\"\"\n",
    "    def _init():\n",
    "        env = make_synth_env(\n",
    "            H_min=H_min, \n",
    "            H_max=H_max,\n",
    "            worker_ratio=worker_ratio,\n",
    "            seed=seed + rank,\n",
    "            verbose=False,\n",
    "            use_batch_arrival=True,  # Use new features\n",
    "            use_capacity_ramp=True,\n",
    "        )\n",
    "        return Monitor(env)\n",
    "    return _init\n",
    "\n",
    "def make_train_vec_env(seed: int, n_envs: int) -> VecNormalize:\n",
    "    \"\"\"Create vectorized training environments with normalization.\"\"\"\n",
    "    env_fns = [\n",
    "        _make_env_worker(\n",
    "            H_min=H_RANGE[0], \n",
    "            H_max=H_RANGE[1],\n",
    "            worker_ratio=WORKER_RATIO,\n",
    "            seed=seed,\n",
    "            rank=i\n",
    "        )\n",
    "        for i in range(n_envs)\n",
    "    ]\n",
    "    \n",
    "    # Try SubprocVecEnv first, fallback to DummyVecEnv\n",
    "    try:\n",
    "        # Smoke test\n",
    "        _tmp = env_fns[0]()\n",
    "        _tmp.close()\n",
    "        vec = SubprocVecEnv(env_fns, start_method=\"spawn\")\n",
    "        print(f\"  ‚úì Using SubprocVecEnv with {n_envs} workers\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è SubprocVecEnv failed, using DummyVecEnv: {e}\")\n",
    "        vec = DummyVecEnv(env_fns)\n",
    "    \n",
    "    vec = VecMonitor(vec)\n",
    "    vec = VecNormalize(vec, norm_obs=True, norm_reward=True, clip_obs=10.0, clip_reward=10.0)\n",
    "    return vec\n",
    "\n",
    "def make_eval_vec_env(seed: int) -> VecNormalize:\n",
    "    \"\"\"Create evaluation environment.\"\"\"\n",
    "    eval_env_fns = [\n",
    "        _make_env_worker(\n",
    "            H_min=H_RANGE[0], \n",
    "            H_max=H_RANGE[1],\n",
    "            worker_ratio=WORKER_RATIO,\n",
    "            seed=seed + 9999,\n",
    "            rank=0\n",
    "        )\n",
    "    ]\n",
    "    eval_vec = DummyVecEnv(eval_env_fns)\n",
    "    eval_vec = VecMonitor(eval_vec)\n",
    "    eval_vec = VecNormalize(\n",
    "        eval_vec, \n",
    "        norm_obs=True, \n",
    "        norm_reward=True, \n",
    "        clip_obs=10.0, \n",
    "        clip_reward=10.0, \n",
    "        training=False\n",
    "    )\n",
    "    return eval_vec\n",
    "\n",
    "print(\"‚úì Environment factory functions defined\")\n",
    "print(\"  - Training: Synthetic environments with batch arrival & capacity ramp\")\n",
    "print(\"  - Evaluation: Same setup for consistent comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Training Setup & Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Main Training Loop ========================\n",
    "\n",
    "def train_agent(seed: int, quick_test: bool = False):\n",
    "    \"\"\"Train a single SAC agent.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training with seed={seed}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    set_random_seed(seed)\n",
    "    ts_tag = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    run_name = f\"sac_synth_seed{seed}_{ts_tag}\"\n",
    "    if quick_test:\n",
    "        run_name = f\"TEST_{run_name}\"\n",
    "    \n",
    "    RUN_DIR = ROOT_RUNS / run_name\n",
    "    TB_LOG  = RUN_DIR / \"tensorboard_logs\" / \"SAC_1\"\n",
    "    RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"üìÅ Run directory: {RUN_DIR}\")\n",
    "    \n",
    "    # Create environments\n",
    "    print(f\"\\nüèóÔ∏è Creating environments...\")\n",
    "    train_vec = make_train_vec_env(seed, N_ENVS)\n",
    "    eval_vec  = make_eval_vec_env(seed)\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    TOTAL = int(TOTAL_STEPS)\n",
    "    if quick_test:\n",
    "        # Simple constant learning rate for quick test\n",
    "        def lr_schedule(progress_remaining: float) -> float:\n",
    "            return 3e-4\n",
    "    else:\n",
    "        # Staged learning rate for full training\n",
    "        B1, B2 = int(0.60 * TOTAL), int(0.85 * TOTAL)\n",
    "        LR1, LR2, LR3 = 3e-4, 1e-4, 5e-5\n",
    "        def lr_schedule(progress_remaining: float) -> float:\n",
    "            step_done = int((1.0 - progress_remaining) * TOTAL)\n",
    "            if step_done < B1:   return LR1\n",
    "            elif step_done < B2: return LR2\n",
    "            else:                return LR3\n",
    "    \n",
    "    # Policy network architecture\n",
    "    # Increased size for M=512 (2054-dim observations)\n",
    "    policy_kwargs = dict(\n",
    "        net_arch=dict(\n",
    "            pi=[512, 512],  # Actor network\n",
    "            qf=[512, 512]   # Critic network\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    batch_size = 256 if quick_test else (1024 if DEVICE == \"mps\" else 512)\n",
    "    \n",
    "    print(f\"\\nü§ñ Creating SAC model...\")\n",
    "    print(f\"  Observation space: {train_vec.observation_space.shape}\")\n",
    "    print(f\"  Action space: {train_vec.action_space.shape}\")\n",
    "    print(f\"  Policy network: {policy_kwargs}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Device: {DEVICE}\")\n",
    "    \n",
    "    model = SAC(\n",
    "        \"MlpPolicy\",\n",
    "        train_vec,\n",
    "        verbose=1,\n",
    "        device=DEVICE,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        learning_rate=lr_schedule,\n",
    "        buffer_size=min(10_000, TOTAL) if quick_test else max(300_000, TOTAL),\n",
    "        batch_size=batch_size,\n",
    "        gamma=0.95,\n",
    "        tau=0.01,\n",
    "        train_freq=(1, \"step\"),\n",
    "        gradient_steps=1,\n",
    "        ent_coef=\"auto\",\n",
    "        tensorboard_log=str(TB_LOG),\n",
    "        seed=seed,\n",
    "    )\n",
    "    \n",
    "    # Setup callbacks\n",
    "    sync_cb = SyncVecNormCallback(src_vecnorm=train_vec, tgt_vecnorm=eval_vec)\n",
    "    eval_cb = EvalCallback(\n",
    "        eval_vec,\n",
    "        best_model_save_path=str(RUN_DIR / \"best\"),\n",
    "        log_path=str(RUN_DIR / \"eval\"),\n",
    "        eval_freq=max(1, EVAL_FREQ // max(1, N_ENVS)),\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "    )\n",
    "    ckpt_cb = CheckpointCallback(\n",
    "        save_freq=max(1, CKPT_FREQ // max(1, N_ENVS)),\n",
    "        save_path=str(RUN_DIR / \"ckpt\"),\n",
    "        name_prefix=\"sac\"\n",
    "    )\n",
    "    completion_cb = CompletionTBCallback(tb_every=200)\n",
    "    \n",
    "    # Start training\n",
    "    print(f\"\\nüöÄ Starting training for {TOTAL_STEPS:,} steps...\")\n",
    "    if quick_test:\n",
    "        print(\"  (Quick test mode - just verifying functionality)\")\n",
    "    \n",
    "    model.learn(\n",
    "        total_timesteps=TOTAL_STEPS,\n",
    "        callback=[sync_cb, eval_cb, ckpt_cb, completion_cb],\n",
    "        progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Save model and stats\n",
    "    model_path = RUN_DIR / \"sac_model.zip\"\n",
    "    model.save(str(model_path))\n",
    "    \n",
    "    vecnorm_path = RUN_DIR / \"vecnormalize.pkl\"\n",
    "    train_vec.save(str(vecnorm_path))\n",
    "    \n",
    "    print(f\"\\n‚úÖ Training complete!\")\n",
    "    print(f\"  Model saved: {model_path}\")\n",
    "    print(f\"  VecNormalize stats: {vecnorm_path}\")\n",
    "    print(f\"  Best model: {RUN_DIR / 'best'}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    try:\n",
    "        train_vec.close()\n",
    "        eval_vec.close()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return model_path\n",
    "\n",
    "# Run training\n",
    "print(\"Ready to train!\")\n",
    "print(f\"\\nSettings:\")\n",
    "print(f\"  Quick test mode: {QUICK_TEST}\")\n",
    "print(f\"  Seeds: {SEEDS}\")\n",
    "print(f\"  Total steps per seed: {TOTAL_STEPS:,}\")\n",
    "print(f\"  Parallel environments: {N_ENVS}\")\n",
    "print(\"\\nCall train_agent(seed) to start training.\")\n",
    "\n",
    "if QUICK_TEST:\n",
    "    print(\"\\nüí° TIP: Since QUICK_TEST=True, training will be very fast (1-2 episodes)\")\n",
    "    print(\"     This is just to verify the system works correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Execute Training ========================\n",
    "\n",
    "saved_models = []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    model_path = train_agent(seed, quick_test=QUICK_TEST)\n",
    "    saved_models.append(model_path)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSaved {len(saved_models)} model(s):\")\n",
    "for p in saved_models:\n",
    "    print(f\"  - {p}\")\n",
    "\n",
    "if QUICK_TEST:\n",
    "    print(\"\\n‚ö†Ô∏è Note: This was a quick test run.\")\n",
    "    print(\"   Set QUICK_TEST=False in Cell 2 for full training.\")\n",
    "else:\n",
    "    print(\"\\nüí° Next steps:\")\n",
    "    print(\"   1. Check tensorboard logs: tensorboard --logdir runs/\")\n",
    "    print(\"   2. Evaluate on real data: python evaluate.py\")\n",
    "    print(\"   3. Load model for inference:\")\n",
    "    print(\"      model = SAC.load('runs/.../sac_model.zip')\")\n",
    "    print(\"      vecnorm = VecNormalize.load('runs/.../vecnormalize.pkl')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Quick Evaluation Test\n",
    "\n",
    "Test the trained model on a synthetic environment to verify it learned something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Quick Evaluation ========================\n",
    "\n",
    "if saved_models:\n",
    "    print(\"=\"*60)\n",
    "    print(\"QUICK EVALUATION TEST\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load the last trained model\n",
    "    model_path = saved_models[-1]\n",
    "    print(f\"\\nüìÇ Loading model: {model_path}\")\n",
    "    \n",
    "    try:\n",
    "        model = SAC.load(str(model_path))\n",
    "        print(\"‚úì Model loaded successfully\")\n",
    "        \n",
    "        # Create test environment\n",
    "        test_env = make_synth_env(\n",
    "            H_min=2000,\n",
    "            H_max=3000,\n",
    "            worker_ratio=0.15,\n",
    "            seed=999,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Compare RL vs baselines\n",
    "        from evaluate import rollout\n",
    "        \n",
    "        print(\"\\nüèÉ Running rollouts (100 days each)...\")\n",
    "        \n",
    "        policies = [\"SAC\", \"LJF\", \"SJF\", \"Random\"]\n",
    "        results = {}\n",
    "        \n",
    "        for policy in policies:\n",
    "            if policy == \"SAC\":\n",
    "                traj = rollout(test_env, model=model, max_days=100)\n",
    "            else:\n",
    "                baseline_env = create_baseline_env(\n",
    "                    region_key=test_env.region_key,\n",
    "                    policy=policy,\n",
    "                    num_contractors=test_env.num_contractors,\n",
    "                    seed=999\n",
    "                )\n",
    "                traj = rollout(baseline_env, max_days=100)\n",
    "            \n",
    "            results[policy] = traj\n",
    "            final = traj[-1] if len(traj) > 0 else 0.0\n",
    "            print(f\"  {policy:8s}: Day 100 completion = {final:.2%}\")\n",
    "        \n",
    "        # Plot comparison\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for policy, traj in results.items():\n",
    "            plt.plot(traj, label=policy, alpha=0.8)\n",
    "        \n",
    "        plt.xlabel(\"Day\")\n",
    "        plt.ylabel(\"Completion\")\n",
    "        plt.title(\"Policy Comparison on Synthetic Test\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n‚úÖ Evaluation complete!\")\n",
    "        \n",
    "        if QUICK_TEST:\n",
    "            print(\"\\n‚ö†Ô∏è Note: Model was only trained for 1000 steps.\")\n",
    "            print(\"   Don't expect good performance yet!\")\n",
    "            print(\"   This just verifies the system works.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during evaluation: {e}\")\n",
    "else:\n",
    "    print(\"No models to evaluate. Run training first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates the new HouseGym RL architecture with:\n",
    "\n",
    "‚úÖ **Pure random candidate selection** (M=512)  \n",
    "‚úÖ **Batch arrival system** (houses revealed over time)  \n",
    "‚úÖ **Capacity ramp system** (contractors mobilize gradually)  \n",
    "‚úÖ **Training on synthetic data**  \n",
    "‚úÖ **Testing on real data** (via evaluate.py)  \n",
    "\n",
    "The diagnostic tests show that:\n",
    "- Candidates are selected randomly (no bias)\n",
    "- LJF sorts by descending total work\n",
    "- SJF sorts by ascending total work  \n",
    "- RL learns to score and rank candidates\n",
    "\n",
    "The focus is on **robustness**, not optimization!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urbanai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
