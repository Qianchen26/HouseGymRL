{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d384eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== MAIN: Train + Evaluate(7 regions) + Report ======================\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- Project modules (不要 reload(config)，避免 OUTPUT_DIR 时间戳漂移)\n",
    "import config\n",
    "import evaluate\n",
    "import housegymrl\n",
    "from evaluate import create_unified_ramp, create_tasks_from_real_config\n",
    "from housegymrl import RLEnv, BaselineEnv  # 需已按我们约定实现：两个环境类\n",
    "\n",
    "# ---- SB3\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv, VecMonitor, VecNormalize\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "# 热更新非 config 文件\n",
    "reload(evaluate)\n",
    "reload(housegymrl)\n",
    "\n",
    "# ----------------- 全局参数 -----------------\n",
    "SEEDS = [42]\n",
    "N_ENVS = 8\n",
    "TOTAL_STEPS = 300_000\n",
    "EVAL_FREQ = 10_000\n",
    "CKPT_FREQ = 50_000\n",
    "TRAIN_REGION_KEYS = list(config.REGION_CONFIG.keys())  # 覆盖全部区域\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "ROOT_RUNS = Path(\"runs\"); ROOT_RUNS.mkdir(exist_ok=True, parents=True)\n",
    "try:\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "except RuntimeError:\n",
    "    pass\n",
    "\n",
    "# ----------------- TensorBoard 记录器 -----------------\n",
    "class CompletionTBCallback(BaseCallback):\n",
    "    def __init__(self, tb_every: int = 200, verbose: int = 0):\n",
    "        super().__init__(verbose); self.tb_every = int(tb_every)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        infos = self.locals.get(\"infos\", None) or []\n",
    "        # 记录 makespan（回合级，env 内已在 done=true 时写入 info['day']）\n",
    "        done_days = [i.get(\"day\") for i in infos if isinstance(i, dict) and i.get(\"done\") is True]\n",
    "        if done_days:\n",
    "            self.logger.record(\"env/makespan_episode\", float(np.min(done_days)))\n",
    "            self.logger.record(\"env/episodes_finished\", float(len(done_days)))\n",
    "\n",
    "        if self.tb_every > 0 and (self.num_timesteps % self.tb_every != 0):\n",
    "            return True\n",
    "        if not infos:\n",
    "            return True\n",
    "\n",
    "        vals = [i.get(\"completion\") for i in infos if isinstance(i, dict) and (\"completion\" in i)]\n",
    "        if not vals:\n",
    "            return True\n",
    "\n",
    "        idle_vals = [i.get(\"idle_workers\", 0) for i in infos if isinstance(i, dict)]\n",
    "        K_vals    = [i.get(\"K_effective\", 0) for i in infos if isinstance(i, dict)]\n",
    "        alloc_vals= [i.get(\"allocated_workers\", 0) for i in infos if isinstance(i, dict)]\n",
    "        util_vals = [a / max(1, k) for a, k in zip(alloc_vals, K_vals)] if K_vals else []\n",
    "\n",
    "        v = np.asarray(vals, dtype=float)\n",
    "        self.logger.record(\"env/completion_household_mean\", float(np.nanmean(v)))\n",
    "        self.logger.record(\"env/completion_household_min\",  float(np.nanmin(v)))\n",
    "        self.logger.record(\"env/completion_household_max\",  float(np.nanmax(v)))\n",
    "        self.logger.record(\"env/completion_household_p50\",  float(np.nanpercentile(v, 50)))\n",
    "        self.logger.record(\"env/completion_household_p90\",  float(np.nanpercentile(v, 90)))\n",
    "\n",
    "        if idle_vals: self.logger.record(\"env/idle_workers_mean\", float(np.mean(idle_vals)))\n",
    "        if util_vals: self.logger.record(\"env/utilization_mean\", float(np.mean(util_vals)))\n",
    "        if alloc_vals:\n",
    "            alloc_arr = np.asarray(alloc_vals, dtype=float)\n",
    "            self.logger.record(\"env/allocated_workers_p50\", float(np.percentile(alloc_arr, 50)))\n",
    "            self.logger.record(\"env/allocated_workers_p90\", float(np.percentile(alloc_arr, 90)))\n",
    "        if K_vals:\n",
    "            K_arr = np.asarray(K_vals, dtype=float)\n",
    "            self.logger.record(\"env/K_effective_p50\", float(np.percentile(K_arr, 50)))\n",
    "            self.logger.record(\"env/K_effective_p90\", float(np.percentile(K_arr, 90)))\n",
    "\n",
    "        if hasattr(self.model, \"actor\") and getattr(self.model.actor, \"optimizer\", None) is not None:\n",
    "            lr = self.model.actor.optimizer.param_groups[0][\"lr\"]\n",
    "            self.logger.record(\"train/lr\", float(lr))\n",
    "        if hasattr(self.model, \"log_ent_coef\"):\n",
    "            ent_coef = float(self.model.log_ent_coef.exp().item())\n",
    "            self.logger.record(\"train/ent_coef\", ent_coef)\n",
    "        return True\n",
    "\n",
    "# ----------------- Ramp -----------------\n",
    "ramp_fn = create_unified_ramp()\n",
    "\n",
    "# ----------------- 训练用 sampler -----------------\n",
    "# ----------------- 训练用 sampler -----------------\n",
    "\n",
    "USE_SYNTHETIC_TRAIN = getattr(config, \"USE_SYNTHETIC_TRAIN\", False)\n",
    "\n",
    "def make_sampler(seed: int):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    def sampler():\n",
    "        region_key = rng.choice(TRAIN_REGION_KEYS)\n",
    "        cfg = config.REGION_CONFIG[region_key]\n",
    "        if USE_SYNTHETIC_TRAIN and hasattr(evaluate, \"create_tasks_from_synthetic\"):\n",
    "            tasks_df = evaluate.create_tasks_from_synthetic(cfg, rng)\n",
    "        else:\n",
    "            tasks_df = create_tasks_from_real_config(cfg, rng)\n",
    "        resources = {\"workers\": int(cfg[\"num_contractors\"]), \"region_name\": region_key}\n",
    "        return tasks_df, resources, {\"region\": region_key}\n",
    "    return sampler\n",
    "\n",
    "\n",
    "def make_train_env(seed: int):\n",
    "    sampler = make_sampler(seed)\n",
    "    def _init():\n",
    "        env = RLEnv(\n",
    "            scenario_sampler=sampler,\n",
    "            M=config.M_CANDIDATES,\n",
    "            max_steps=config.MAX_STEPS,\n",
    "            seed=seed,\n",
    "            k_ramp=ramp_fn,\n",
    "            batch_arrival=True,\n",
    "        )\n",
    "        return Monitor(env)\n",
    "    return _init\n",
    "\n",
    "def make_eval_env(region_key: str, seed: int, env_cls):\n",
    "    def _init():\n",
    "        env = evaluate.make_region_env(region_key, env_cls, k_ramp=ramp_fn, batch_arrival=True)\n",
    "        return Monitor(env)\n",
    "    return _init\n",
    "\n",
    "# ----------------- 使用训练统计包裹单环境（推理） -----------------\n",
    "def wrap_single_env_with_vecnorm(env, src_vecnorm):\n",
    "    dummy = DummyVecEnv([lambda: env])\n",
    "    v = VecNormalize(dummy, norm_obs=True, norm_reward=False, training=False, clip_obs=src_vecnorm.clip_obs)\n",
    "    v.obs_rms = src_vecnorm.obs_rms\n",
    "    v.clip_obs = src_vecnorm.clip_obs\n",
    "    v.training = False\n",
    "    v.norm_reward = False\n",
    "    v.ret_rms = src_vecnorm.ret_rms\n",
    "    return v\n",
    "\n",
    "# ----------------- Baseline 分配策略（整数动作） -----------------\n",
    "def _baseline_order(view, policy: str, rng: random.Random):\n",
    "    \"\"\"\n",
    "    view: env.last_candidate_view() 的返回，期望包含：\n",
    "      idx, cmax, mask, damage_level(0/1/2), arr_rem(剩余工日) 等\n",
    "    返回：候选在 slate 中的访问顺序（np.ndarray of indices）\n",
    "    \"\"\"\n",
    "    idx  = view.get(\"idx\"); cmax = view.get(\"cmax\"); mask = view.get(\"mask\")\n",
    "    valid = (mask > 0) & (idx >= 0) if (idx is not None and mask is not None) else None\n",
    "    order = np.where(valid)[0] if valid is not None else np.arange(len(idx) if idx is not None else 0)\n",
    "\n",
    "    # 需要的属性\n",
    "    damage = view.get(\"damage_level\", None)\n",
    "    # [DEBUG FIX] 避免对 ndarray 使用 or 带来“数组真值不明确”\n",
    "    rem = view.get(\"arr_rem\", None)\n",
    "    if rem is None:\n",
    "        rem = view.get(\"rem_days\", None)\n",
    "    if rem is None:\n",
    "        rem = view.get(\"remaining_days\", None)\n",
    "    if rem is None:\n",
    "        # 若缺失剩余工日，用 cmax 替代一个 proxy\n",
    "        rem = cmax if cmax is not None else np.ones_like(order)\n",
    "\n",
    "    if policy == \"FIFO\":\n",
    "        return order\n",
    "\n",
    "    if policy == \"RANDOM\":\n",
    "        arr = order.copy()\n",
    "        rng.shuffle(arr)\n",
    "        return arr\n",
    "\n",
    "    # SJF：剩余工日升序；LJF：重灾优先 + 同级按剩余工日降序\n",
    "    if policy == \"SJF\":\n",
    "        if rem is None: return order\n",
    "        # 有些 candidate 可能 rem 为负/NaN，先处理\n",
    "        remv = np.asarray(rem, dtype=float)\n",
    "        remv = np.where(np.isfinite(remv), remv, np.inf)\n",
    "        return order[np.argsort(remv[order])]\n",
    "\n",
    "    if policy == \"LJF\":\n",
    "        # 「重灾(LJF)先、其余(中/轻)随机」；重灾内部按剩余工日降序\n",
    "        if damage is None:  # 若没 damage_level，则退化为全体按剩余工日降序\n",
    "            remv = np.asarray(rem, dtype=float)\n",
    "            remv = np.where(np.isfinite(remv), remv, -np.inf)\n",
    "            return order[np.argsort(-remv[order])]\n",
    "        damage = np.asarray(damage)\n",
    "        major = order[damage[order] == 2]\n",
    "        others = order[damage[order] != 2]\n",
    "        # major: 剩余工日降序\n",
    "        remv = np.asarray(rem, dtype=float)\n",
    "        remv = np.where(np.isfinite(remv), remv, -np.inf)\n",
    "        major_sorted = major[np.argsort(-remv[major])] if major.size else major\n",
    "        # others: 随机\n",
    "        others_shuf = others.copy()\n",
    "        rng.shuffle(others_shuf)\n",
    "        return np.concatenate([major_sorted, others_shuf])\n",
    "\n",
    "    # fallback\n",
    "    return order\n",
    "\n",
    "def _baseline_alloc(env, policy: str, rng: random.Random):\n",
    "    # NOTE: BaselineEnv 下 action=每个候选的整数分配\n",
    "    c = env.last_candidate_view()\n",
    "    idx, cmax, mask = c[\"idx\"], c[\"cmax\"], c[\"mask\"]\n",
    "    order = _baseline_order(c, policy, rng)\n",
    "    K_eff = int(env.k_ramp(env.day) * env.K) if getattr(env, \"k_ramp\", None) else env.K\n",
    "    alloc = np.zeros_like(idx, dtype=np.int32)\n",
    "    for i in order:\n",
    "        if K_eff <= 0: break\n",
    "        give = min(int(cmax[i]), K_eff)\n",
    "        alloc[i] = give\n",
    "        K_eff -= give\n",
    "    return alloc\n",
    "\n",
    "# ----------------- 指标函数 -----------------\n",
    "def compute_auc(curve: np.ndarray, T: int) -> float:\n",
    "    T = int(T)\n",
    "    if T <= 0 or curve.size == 0: return 0.0\n",
    "    c = curve[:min(T, len(curve))]\n",
    "    return float(np.clip(np.trapz(c, dx=1) / T, 0.0, 1.0))\n",
    "\n",
    "def compute_t_percentile(curve: np.ndarray, p: float) -> int:\n",
    "    if curve.size == 0: return np.inf\n",
    "    idx = np.where(curve >= p)[0]\n",
    "    return int(idx[0]) if idx.size > 0 else np.inf\n",
    "\n",
    "def compute_utilization(idle_list, K_total) -> float:\n",
    "    if not idle_list or K_total <= 0: return 0.0\n",
    "    return float(np.clip(1.0 - (np.mean(idle_list) / K_total), 0.0, 1.0))\n",
    "\n",
    "def rmse_pair(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    n = min(len(a), len(b))\n",
    "    if n == 0: return np.nan\n",
    "    aa = a[:n]; bb = b[:n]\n",
    "    m = np.isfinite(bb)\n",
    "    if m.any():\n",
    "        aa = aa[m[:len(aa)]]\n",
    "        bb = bb[m]\n",
    "    if len(aa) == 0: return np.nan\n",
    "    return float(np.sqrt(np.mean((aa - bb)**2)))\n",
    "\n",
    "# ----------------- 新增：逐日 K_effective 的利用率（与训练端口径一致） -----------------\n",
    "def compute_utilization_from_series(idle_list, k_eff_list, fallback_K: int | None = None) -> float:\n",
    "    if not idle_list or not k_eff_list:\n",
    "        return 0.0 if fallback_K is None else compute_utilization(idle_list, fallback_K)\n",
    "    n = min(len(idle_list), len(k_eff_list))\n",
    "    if n == 0:\n",
    "        return 0.0 if fallback_K is None else compute_utilization(idle_list, fallback_K)\n",
    "    # 若全部 k_eff<=0，回退到常数 K 口径（保证结果稳定而不是硬 0）\n",
    "    if all((int(k) if np.isfinite(k) else 0) <= 0 for k in k_eff_list[:n]):\n",
    "        return 0.0 if fallback_K is None else compute_utilization(idle_list[:n], fallback_K)\n",
    "    vals = []\n",
    "    for i in range(n):\n",
    "        k = int(k_eff_list[i]) if np.isfinite(k_eff_list[i]) else 0\n",
    "        k = max(1, k)\n",
    "        vals.append(1.0 - (float(idle_list[i]) / k))\n",
    "    return float(np.clip(np.mean(vals), 0.0, 1.0))\n",
    "\n",
    "\n",
    "# ----------------- rollout -----------------\n",
    "def rollout_region(env, model=None, policy=\"SAC\", max_days=2000, seed=0):\n",
    "    \"\"\"\n",
    "    兼容两种 API：\n",
    "    - Gymnasium 单环境：reset()->(obs, info), step()->(obs, reward, terminated, truncated, info)\n",
    "    - VecEnv（DummyVecEnv/VecNormalize）：reset()->obs, step()->(obs, rewards, dones, infos)\n",
    "    \"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    # ---- 是否是 VecEnv（如 DummyVecEnv/VecNormalize）----\n",
    "    vec_mode = hasattr(env, \"num_envs\") or hasattr(env, \"venv\")\n",
    "\n",
    "    # ---- reset 兼容 ----\n",
    "    _res = env.reset()\n",
    "    if isinstance(_res, tuple) and len(_res) == 2:\n",
    "        obs, info = _res\n",
    "    else:\n",
    "        obs, info = _res, {}\n",
    "\n",
    "    curve, idle_hist, k_eff_hist = [], [], []\n",
    "    done_day = None\n",
    "\n",
    "    for _ in range(max_days):\n",
    "        # 动作\n",
    "        if policy == \"SAC\":\n",
    "            act, _ = model.predict(obs, deterministic=True)\n",
    "            # VecEnv 需要 batch 维度\n",
    "            if vec_mode and isinstance(act, np.ndarray) and act.ndim == 1:\n",
    "                act = act[None, :]\n",
    "        else:\n",
    "            # Baseline 只在非 VecEnv 路径跑（此处也做了兜底以防万一）\n",
    "            base_env = getattr(env, \"unwrapped\", env)\n",
    "            alloc = _baseline_alloc(base_env, policy, rng)\n",
    "            act = alloc\n",
    "            if vec_mode:\n",
    "                act = np.asarray(act)[None, :]\n",
    "\n",
    "        if vec_mode:\n",
    "            # VecEnv: step -> (obs, rewards, dones, infos)\n",
    "            o, r, d, infos = env.step(act)\n",
    "            info = infos[0] if isinstance(infos, (list, tuple)) and len(infos) else {}\n",
    "            obs = o  # 仍保持 (1, obs_dim) 的 batch 形状\n",
    "\n",
    "            curve.append(info.get(\"completion\", 0.0))\n",
    "            idle_hist.append(info.get(\"idle_workers\", 0))\n",
    "            k_eff_hist.append(info.get(\"K_effective\", 0))\n",
    "\n",
    "            done_flag = bool(np.asarray(d)[0])\n",
    "            if info.get(\"done\") is True or done_flag:\n",
    "                done_day = info.get(\"day\", len(curve) - 1)\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        # 单环境（Gymnasium）路径\n",
    "        obs, reward, terminated, truncated, info = env.step(act)\n",
    "        curve.append(info.get(\"completion\", 0.0))\n",
    "        idle_hist.append(info.get(\"idle_workers\", 0))\n",
    "        k_eff_hist.append(info.get(\"K_effective\", 0))\n",
    "\n",
    "        \n",
    "        if info.get(\"done\") is True:\n",
    "            done_day = info.get(\"day\")\n",
    "            break\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    done_day = (done_day if done_day is not None else len(curve) - 1)\n",
    "    return np.array(curve, dtype=float), idle_hist, k_eff_hist, int(done_day)\n",
    "\n",
    "\n",
    "# ----------------- [DEBUG FIX] 环境属性安全读取工具 -----------------\n",
    "def _safe_total_houses(e):\n",
    "    ue = getattr(e, \"unwrapped\", e)\n",
    "    # 尝试常见“标量”属性名\n",
    "    for name in (\"H_total\", \"n_tasks\", \"num_tasks\", \"num_houses\", \"n_houses\", \"N\", \"H\", \"total_houses\"):\n",
    "        if hasattr(ue, name):\n",
    "            try:\n",
    "                return int(getattr(ue, name))\n",
    "            except Exception:\n",
    "                pass\n",
    "    # 再尝试容器/表：tasks/houses/DataFrame\n",
    "    for name in (\"tasks\", \"houses\", \"tasks_df\", \"houses_df\"):\n",
    "        if hasattr(ue, name):\n",
    "            obj = getattr(ue, name)\n",
    "            try:\n",
    "                return int(obj.shape[0]) if hasattr(obj, \"shape\") else int(len(obj))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return np.nan  # 实在拿不到就 NaN（不会影响后续 CSV 输出）\n",
    "\n",
    "def _safe_K(e):\n",
    "    ue = getattr(e, \"unwrapped\", e)\n",
    "    for name in (\"K\", \"K_base\", \"num_workers\", \"num_contractors\", \"workers\"):\n",
    "        if hasattr(ue, name):\n",
    "            try:\n",
    "                return int(getattr(ue, name))\n",
    "            except Exception:\n",
    "                pass\n",
    "    # 正确的 resources fallback（ue.resources 是 dict）\n",
    "    if hasattr(ue, \"resources\"):\n",
    "        res = getattr(ue, \"resources\")\n",
    "        if isinstance(res, dict) and \"workers\" in res:\n",
    "            try:\n",
    "                return int(res[\"workers\"])\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return 0\n",
    "\n",
    "# ----------------- 新增：评估时核对户数与配置一致性（只告警） -----------------\n",
    "def _warn_h_total_mismatch(region_key: str, H_env: int):\n",
    "    try:\n",
    "        exp = int(np.sum(config.REGION_CONFIG[region_key].get(\"damage_dist\", [])))\n",
    "        if H_env == H_env and exp > 0 and H_env != exp:\n",
    "            print(f\"[WARN] {region_key}: env houses={H_env} != sum(config.damage_dist)={exp}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# ----------------- 单 region 评估（含 observed 曲线的 RMSE，若可用） -----------------\n",
    "def evaluate_region(region_key, model, vecnorm_src, max_days=2000, seed=1234):\n",
    "    result_rows = []\n",
    "\n",
    "    # 1) RL (RLEnv + VecNorm 统计)\n",
    "    rl_env = make_eval_env(region_key, seed=seed, env_cls=RLEnv)()\n",
    "    rl_env_v = wrap_single_env_with_vecnorm(rl_env, vecnorm_src)\n",
    "    curve, idle, k_eff, done_day = rollout_region(rl_env_v, model=model, policy=\"SAC\", max_days=max_days, seed=seed)\n",
    "    # [DEBUG FIX] 安全获取 K 和总户数\n",
    "    rl_K = _safe_K(rl_env)\n",
    "    rl_H = _safe_total_houses(rl_env)\n",
    "    _warn_h_total_mismatch(region_key, rl_H)\n",
    "    result_rows.append((\"SAC\", curve, idle, k_eff, done_day, rl_K, rl_H))\n",
    "\n",
    "\n",
    "    # 2) Baselines：SJF / LJF / RANDOM\n",
    "    for pol in (\"SJF\", \"LJF\", \"RANDOM\"):\n",
    "        bl_env = make_eval_env(region_key, seed=seed+111, env_cls=BaselineEnv)()\n",
    "        c2, idle2, k2, done2 = rollout_region(bl_env, model=None, policy=pol, max_days=max_days, seed=seed+111)\n",
    "        # [DEBUG FIX] 安全获取 K 和总户数\n",
    "        bl_K = _safe_K(bl_env)\n",
    "        bl_H = _safe_total_houses(bl_env)\n",
    "        _warn_h_total_mismatch(region_key, bl_H)\n",
    "        result_rows.append((pol, c2, idle2, k2, done2, bl_K, bl_H))\n",
    "\n",
    "\n",
    "    # 3) 若 evaluate.py 提供 observed 曲线处理，则加载它用于 RMSE\n",
    "    obs_curve = None\n",
    "    try:\n",
    "        if hasattr(evaluate, \"process_observed_with_nan\"):\n",
    "            obs_curve = evaluate.process_observed_with_nan(region_key)\n",
    "            # 允许返回 dict 或 ndarray\n",
    "            if isinstance(obs_curve, dict):\n",
    "                # 常见字段名猜测：'curve', 'completion', 'y'\n",
    "                for k in (\"curve\", \"completion\", \"y\"):\n",
    "                    if k in obs_curve:\n",
    "                        obs_curve = obs_curve[k]; break\n",
    "            obs_curve = np.asarray(obs_curve, dtype=float)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] observed load failed for {region_key}: {e}\")\n",
    "        obs_curve = None\n",
    "\n",
    "    # 4) 汇总指标\n",
    "    out = []\n",
    "    for policy, c, idle_hist, k_eff_hist, done, K, H_total in result_rows:\n",
    "        row = {\n",
    "            \"region\": region_key,\n",
    "            \"strategy\": policy,\n",
    "            \"makespan\": int(done),\n",
    "            \"auc@200\": compute_auc(c, 200),\n",
    "            \"auc@300\": compute_auc(c, 300),\n",
    "            \"t80\": compute_t_percentile(c, 0.80),\n",
    "            \"t90\": compute_t_percentile(c, 0.90),\n",
    "            \"t95\": compute_t_percentile(c, 0.95),\n",
    "            \"utilization\": compute_utilization_from_series(idle_hist, k_eff_hist, fallback_K=K),\n",
    "            \"final_completion\": float(c[-1]) if len(c)>0 else 0.0,\n",
    "            \"rmse_aligned\": np.nan,\n",
    "            \"rmse_overlap\": np.nan,\n",
    "            \"H_total\": int(H_total) if (H_total == H_total) else np.nan,  # NaN 保护\n",
    "            \"K\": int(K),\n",
    "            \"obs_days\": np.nan,\n",
    "            \"obs_final\": np.nan,\n",
    "        }\n",
    "        if obs_curve is not None and len(obs_curve)>0:\n",
    "            row[\"rmse_aligned\"] = rmse_pair(c, obs_curve)\n",
    "            # overlap: 仅在 obs 非 NaN 的区间评估（保持你现有逻辑，不做非必要改动）\n",
    "            mask = np.isfinite(obs_curve)\n",
    "            if mask.any():\n",
    "                row[\"rmse_overlap\"] = rmse_pair(c, obs_curve[mask])\n",
    "            row[\"obs_days\"] = int(np.sum(np.isfinite(obs_curve)))\n",
    "            row[\"obs_final\"] = float(obs_curve[np.isfinite(obs_curve)][-1]) if np.any(np.isfinite(obs_curve)) else np.nan\n",
    "        out.append(row)\n",
    "    return out\n",
    "\n",
    "# ----------------- 报表输出 -----------------\n",
    "def write_summary_and_plots(df: pd.DataFrame, OUT_DIR: Path):\n",
    "    TAB_DIR = OUT_DIR / \"tab\"\n",
    "    FIG_DIR = OUT_DIR / \"fig\"\n",
    "    TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    metrics_path = TAB_DIR / \"metrics_eval.csv\"\n",
    "    df.to_csv(metrics_path, index=False)\n",
    "    print(\"Saved:\", metrics_path)\n",
    "\n",
    "    agg_cols = [\"makespan\",\"auc@200\",\"auc@300\",\"t80\",\"t90\",\"t95\",\"utilization\",\"rmse_aligned\",\"rmse_overlap\",\"final_completion\"]\n",
    "    pivot_table = df.pivot_table(index=\"region\", columns=\"strategy\", values=agg_cols, aggfunc=\"first\")\n",
    "    pivot_flat = pivot_table.copy()\n",
    "    pivot_flat.columns = [f\"{m}__{s}\" for m,s in pivot_flat.columns]\n",
    "    pivot_flat = pivot_flat.reset_index()\n",
    "    summary_csv = TAB_DIR / \"summary_wide.csv\"\n",
    "    pivot_flat.to_csv(summary_csv, index=False)\n",
    "    print(\"Saved:\", summary_csv)\n",
    "\n",
    "    def barplot_metric(metric, ylabel, save_name):\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plot_df = df[[\"region\",\"strategy\",metric]].copy()\n",
    "        means = plot_df.groupby(\"strategy\")[metric].mean().sort_values(ascending=True)\n",
    "        means.plot.bar()\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title(f\"Overall {metric} by Strategy\")\n",
    "        plt.grid(axis=\"y\", alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIG_DIR / save_name, dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "    barplot_metric(\"makespan\", \"Days\", \"overall_makespan.png\")\n",
    "    barplot_metric(\"rmse_aligned\", \"RMSE\", \"overall_rmse_aligned.png\")\n",
    "    barplot_metric(\"rmse_overlap\", \"RMSE\", \"overall_rmse_overlap.png\")\n",
    "    print(\"Saved figures:\",\n",
    "          FIG_DIR / \"overall_makespan.png\",\n",
    "          FIG_DIR / \"overall_rmse_aligned.png\",\n",
    "          FIG_DIR / \"overall_rmse_overlap.png\")\n",
    "\n",
    "    # 单区对比（取第一个 region）\n",
    "    uniq = df[\"region\"].dropna().unique()\n",
    "    if len(uniq):\n",
    "        first_region = uniq[0]\n",
    "        sub = df[df[\"region\"]==first_region].set_index(\"strategy\")\n",
    "        metrics = [\"makespan\",\"auc@200\",\"utilization\",\"rmse_aligned\",\"rmse_overlap\"]\n",
    "        plt.figure(figsize=(12,6))\n",
    "        for i, m in enumerate(metrics, 1):\n",
    "            plt.subplot(1, len(metrics), i)\n",
    "            sub[m].sort_values(ascending=(m in [\"makespan\",\"rmse_aligned\",\"rmse_overlap\"])).plot.bar()\n",
    "            plt.title(m); plt.xticks(rotation=45, ha=\"right\"); plt.grid(axis=\"y\", alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        outp = (FIG_DIR / f\"{first_region}_strategy_compare.png\")\n",
    "        plt.savefig(outp, dpi=200); plt.close()\n",
    "        print(\"Saved region figure:\", outp)\n",
    "\n",
    "    # 榜单（makespan 最优策略）\n",
    "    rank_df = df.pivot_table(index=\"region\", columns=\"strategy\", values=\"makespan\", aggfunc=\"first\")\n",
    "    if not rank_df.empty:\n",
    "        rank_df[\"best_strategy\"] = rank_df.idxmin(axis=1)\n",
    "        display(rank_df.sort_index())\n",
    "\n",
    "# =================== Train + Eval ===================\n",
    "saved_models = []\n",
    "for SEED in SEEDS:\n",
    "    print(f\"========== Training seed={SEED} ==========\")\n",
    "    set_random_seed(SEED)\n",
    "\n",
    "    ts_tag = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    run_dir = ROOT_RUNS / f\"sac_seed{SEED}_{ts_tag}\"\n",
    "    tb_dir  = run_dir / \"tb\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ---- Build train vec env ----\n",
    "    env_fns = [make_train_env(SEED + i) for i in range(N_ENVS)]\n",
    "    _tmp = env_fns[0]() ; _tmp.close()\n",
    "\n",
    "    try:\n",
    "        vec_env = SubprocVecEnv(env_fns, start_method=\"spawn\")\n",
    "    except Exception as exc:\n",
    "        print(\"[WARN] SubprocVecEnv spawn failed, fallback to DummyVecEnv:\", repr(exc))\n",
    "        vec_env = DummyVecEnv(env_fns)\n",
    "    vec_env = VecMonitor(vec_env)\n",
    "    vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "\n",
    "    # ---- Eval env for callback (multi region, RL) ----\n",
    "    def make_eval_envs(seed: int):\n",
    "        envs = []\n",
    "        for i, region_key in enumerate(TRAIN_REGION_KEYS):\n",
    "            envs.append(make_eval_env(region_key, seed + 9000 + i, env_cls=RLEnv))\n",
    "        vec = DummyVecEnv(envs)\n",
    "        vec = VecMonitor(vec)\n",
    "        vec = VecNormalize(vec, norm_obs=True, norm_reward=False, training=False, clip_obs=10.0)\n",
    "        vec.training = False; vec.norm_reward = False\n",
    "        return vec\n",
    "\n",
    "    eval_env = make_eval_envs(SEED)\n",
    "    # 对齐统计\n",
    "    eval_env.obs_rms = vec_env.obs_rms\n",
    "    eval_env.clip_obs = vec_env.clip_obs\n",
    "    eval_env.training = False\n",
    "    eval_env.norm_reward = False\n",
    "    eval_env.ret_rms = vec_env.ret_rms\n",
    "\n",
    "    # ---- manifest ----\n",
    "    manifest = {\n",
    "        \"global_seed\": SEED,\n",
    "        \"train_env_seeds\": [SEED + i for i in range(N_ENVS)],\n",
    "        \"eval_seeds\": {region: SEED + 9000 + idx for idx, region in enumerate(TRAIN_REGION_KEYS)},\n",
    "        \"train_regions\": TRAIN_REGION_KEYS,\n",
    "        \"timestamp\": ts_tag,\n",
    "        \"output_dir\": str(config.OUTPUT_DIR),\n",
    "        \"tensorboard_dir\": str(tb_dir),\n",
    "    }\n",
    "    (run_dir / \"seed_manifest.json\").write_text(json.dumps(manifest, indent=2))\n",
    "\n",
    "    # ---- Policy/LR ----\n",
    "    total = int(TOTAL_STEPS)\n",
    "    b1, b2 = int(0.60 * total), int(0.85 * total)\n",
    "    lr1, lr2, lr3 = 3e-4, 1e-4, 5e-5\n",
    "    def lr_schedule(progress_remaining: float) -> float:\n",
    "        step_done = int((1.0 - progress_remaining) * total)\n",
    "        if step_done < b1: return lr1\n",
    "        if step_done < b2: return lr2\n",
    "        return lr3\n",
    "\n",
    "    policy_kwargs = dict(net_arch=dict(pi=[512], qf=[512]))\n",
    "    batch_size = 1024 if DEVICE == \"mps\" else 512\n",
    "    model = SAC(\n",
    "        \"MlpPolicy\",\n",
    "        vec_env,\n",
    "        verbose=1,\n",
    "        device=DEVICE,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        learning_rate=lr_schedule,\n",
    "        buffer_size=max(300_000, TOTAL_STEPS),\n",
    "        batch_size=batch_size,\n",
    "        gamma=0.95,\n",
    "        tau=0.01,\n",
    "        train_freq=(1, \"step\"),\n",
    "        gradient_steps=1,\n",
    "        ent_coef=\"auto\",\n",
    "        tensorboard_log=str(tb_dir),\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    eval_cb = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=str(run_dir / \"best\"),\n",
    "        log_path=str(run_dir / \"eval\"),\n",
    "        eval_freq=max(1, EVAL_FREQ // max(1, N_ENVS)),\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "    )\n",
    "    ckpt_cb = CheckpointCallback(\n",
    "        save_freq=max(1, CKPT_FREQ // max(1, N_ENVS)),\n",
    "        save_path=str(run_dir / \"ckpt\"),\n",
    "        name_prefix=\"sac\",\n",
    "    )\n",
    "    completion_cb = CompletionTBCallback(tb_every=200)\n",
    "\n",
    "    model.learn(total_timesteps=TOTAL_STEPS, callback=[eval_cb, ckpt_cb, completion_cb], progress_bar=True)\n",
    "\n",
    "    # ---- Save artifacts ----\n",
    "    model_path   = run_dir / \"sac_model.zip\"\n",
    "    vecnorm_path = run_dir / \"vecnormalize.pkl\"\n",
    "    model.save(str(model_path))\n",
    "    vec_env.save(str(vecnorm_path))\n",
    "    saved_models.append(model_path)\n",
    "    print(\"Saved:\", model_path)\n",
    "    print(\"VecNormalize stats:\", vecnorm_path)\n",
    "    print(\"Best checkpoint dir:\", run_dir / \"best\")\n",
    "\n",
    "    # ---- 训练后：对全部 region 评估（RL + baselines）并生成报表 ----\n",
    "    # 强校验动作维度\n",
    "    model_M = model.policy.action_space.shape[0]\n",
    "    assert model_M == config.M_CANDIDATES, f\"Model action dim {model_M} != M_CANDIDATES {config.M_CANDIDATES}\"\n",
    "\n",
    "    OUT_DIR = config.OUTPUT_DIR\n",
    "    (OUT_DIR / \"tab\").mkdir(parents=True, exist_ok=True)\n",
    "    (OUT_DIR / \"fig\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_rows = []\n",
    "    for region_key in TRAIN_REGION_KEYS:\n",
    "        print(f\"[Eval] Region = {region_key}\")\n",
    "        rows = evaluate_region(region_key, model=model, vecnorm_src=vec_env, max_days=config.MAX_STEPS, seed=SEED+2024)\n",
    "        all_rows.extend(rows)\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    write_summary_and_plots(df, OUT_DIR)\n",
    "\n",
    "    # ---- Cleanup ----\n",
    "    try:\n",
    "        vec_env.close(); eval_env.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"All seeds finished. Saved models:\")\n",
    "for path in saved_models:\n",
    "    print(\" -\", path)\n",
    "print(\"Reports saved under:\", config.OUTPUT_DIR)\n",
    "# ====================== END MAIN ======================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9cff88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6e2f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402df61e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606e7c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf96085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urbanai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
